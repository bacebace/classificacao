# -*- coding: utf-8 -*-
"""naive_bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1haYkGFdj7ft5WE4Y6LX4anzOhV2BZSQc

# Naive Bayes

**Recapitulando...**

Todo modelo de machine learning quer estimar:

P(target | variáveis)

Teorema de Bayes:

P(Y|X) = P(X|Y) * P(Y) / P(X)

Temos Y={0,1} e P(X|Y) e P(Y), que são fáceis de calcular.

Além disso, não é necessário calcular P(X), pois:
*  Y={0,1}
*  P(Y=0|X) + P(Y=1|X) = 1
---

**Exemplo**

CASADO (X) / COMPROU (Y)

sim / 1

sim / 1

não / 1

sim / 0

não / 0

não / 1

sim / 1

não / 0

---
P(Y=1) = 5/8

P(Y=0) = 3/8

P(X=sim|Y=1) = 3/5

P(X=não|Y=1) = 2/5

P(X=sim|Y=0) = 1/3

P(X=não|Y=0) = 2/3

---

Utilizando P(Y|X) = P(X|Y) * P(Y) / P(X), temos:

*  Qual a probabilidade de um novo cliente comprar caso seja casado?

P(Y=1|X=sim) = (3/5) * (5/8) / P(X=sim) = (3/8) / P(X=sim)

P(Y=0|X=sim) = (1/3) * (3/8) / P(X=sim) = (1/8) / P(X=sim)

*  Com isso, já é possível concluir que é mais provável que o novo cliente compre!

---

Se você quiser calcular P(X=sim), basta normalizar:

P(Y=1|X=sim) + P(Y=0|X=sim) = 1 ⇒

(3/8) / P(X=sim) + (1/8) / P(X=sim) = 1 ⇒

P(X=sim) = 1/2

*  Probabilidade de comprar caso seja casado: (3/8) / (1/2) = 3/4

*  Probabilidade de não comprar caso seja casado: (1/8) / (1/2) = 1/4

---

Para o caso acima, é simples fazer as contas, mas se há muitas variáveis: calcular P(X1,X2,X3,...|Y) é complicado.

Então vamos fazer a hipótese de que X1,X2,X3,... são  **condicionalmente independentes**: as variáveis X1,X2,X3,... são independentes quando condicionadas a Y.

No exemplo mostrado, seria dizer que quem compra o produto (Y=1) tem um certo perfil descrito por X1,X2,X3,... bastante diferente de quem não compra (Y=0).

Temos:

P(X1,X2,X3,...|Y) = P(X1|Y) * P(X2|Y) * P(X3|Y) * ...

Então, fazendo as substituições:

P(Y|X1,X2,X3,...) = P(X1,X2,X3,...|Y) * P(Y) / P(X1,X2,X3,...) ⇒

P(Y|X1,X2,X3,...) = P(X1|Y) * P(X2|Y) * P(X3|Y) * ... * P(Y) / P(X1,X2,X3,...)


E não precisamos calcular o denominador P(X1,X2,X3,...) para o nosso propósito.

---

**Empiricamente** se notou que, mesmo quando a hipótese de que as variáveis são condicionalmente independentes é falsa, o modelo **ORDENA** bem os dados. Ele não oferece as probabilidades, mas sim uma "propensão" (conceito de *score*).

Se uma pessoa apresenta *score*=0.9, não significa que a probabilidade de o cliente comprar é de 90%, mas sim que a **PROPENSÃO** de ele comprar é maior do que a de um cliente que apresenta *score* de 0.7, por exemplo.

# Categorical naive bayes

**Importar bibliotecas**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import OrdinalEncoder #retorna matriz
from sklearn.naive_bayes import CategoricalNB
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder #retorna lista (usaremos p/ Y)

from google.colab import drive

"""**Criar tabela de dados igual à do exemplo**"""

df = pd.DataFrame(columns=['casado', 'comprou'])

df.loc[0,:] = ['sim', 1]
df.loc[1,:] = ['sim', 1]
df.loc[2,:] = ['não', 1]
df.loc[3,:] = ['sim', 0]
df.loc[4,:] = ['não', 0]
df.loc[5,:] = ['não', 1]
df.loc[6,:] = ['sim', 1]
df.loc[7,:] = ['não', 0]

df

"""**Transformar variáveis de texto em números**"""

enc = OrdinalEncoder() 

enc.fit_transform(df)

"""Vimos que ele transformou 'sim'=1 e 'não'=0.

Ele cria números para categoria em ordem alfabética, então como N vem antes de S, 'não' virou 0 e 'sim' virou 1.

Se houvesse outra palavra como possível resposta para 'casado', como 'talvez', haveria uma terceira categoria, e seria o 3.
"""

enc.categories_

# dataframe preprocessado
# cololocar o datatype (dtype) ajuda o modelo, pois deixa bem definido o que vai ter dentro do dataframe
df_pre = pd.DataFrame(enc.fit_transform(df), columns=df.columns, dtype=int) 
df_pre

"""**Separar variáveis**"""

X = df_pre[['casado']] # com dois colchetes, retorna a tabela

'''
df_pre['casado'] retorna o mesmo que df_pre.casado (vetor)

aqui, queremos usar o que retorna a tabela porque é p/ser
do mesmo jeito que fizemos no módulo de fundamentos.

sempre usamos uma tabela p/carregar as variáveis preditivas.
aqui só temos 1 coluna, mas já vimos antes com várias colunas.
'''

X

Y = df_pre.comprou

'''
como a variável resposta é treinada sozinha (mesmo que haja outras),
então vamos usar o vetor mesmo.
já se espera que seja uma coluna apenas.
'''

Y

"""**Criar modelo naive bayes**"""

cnb = CategoricalNB(alpha=0) # em breve veremos o que esse alpha significa

cnb.fit(X,Y)

"""**Ver resposta do modelo para novos dados**"""

pred = pd.DataFrame(columns=['casado'], dtype=int)
pred.loc[0, :] = [0]
pred.loc[1, :] = [1]

pred

cnb.predict_proba(pred)

"""* Para o novo cliente 0: propensão 0.5 de não comprar, propensão 0.5 de comprar;
* Para o novo cliente 1: propensão 0.25 de não comprar, propensão 0.75 de comprar.

Se olharmos o que foi feito na seção "Naive Bayes" deste notebook, relembraremos:

* Probabilidade de comprar caso seja casado: 3/4

* Probabilidade de não comprar caso seja casado: 1/4

Para nosso cliente 1, que é casado, obtivemos exatamente os mesmos valores calculados anteriormente!

**Outro problema, agora com mais dados**

Vamos testar o modelo para um problema que tem uma base de dados maior.

Aqui, o que queremos descobrir (Y) é se o cliente vai ou não investir dinheiro no nosso banco.
"""

drive.mount('/content/drive')

PATH='drive/My Drive/FLAI/01_classificacao/'
df=pd.read_csv(PATH+'base_categoricalnb.csv', sep=',')

df

"""**Separar conjuntos de treinamento e de teste**"""

X = df.drop('y', axis='columns') # jogar fora a variável resposta
Y = df.y # pegar só a coluna 'y', que é a variável resposta

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=61658)

'''
'random_state' serve para poder reproduzir exatamente os mesmos resultados.
se o random_state é o mesmo (e a versão da biblioteca usada também),
então os resultados que obtenho aqui ou em outro computador devem ser os mesmos.
'''

import sklearn 
sklearn.__version__

print('X_train.shape: ', X_train.shape)
print('X_test.shape: ', X_test.shape)
print('Y_train.shape: ', Y_train.shape)
print('Y_test.shape: ', Y_test.shape)

"""**Transformar variáveis categóricas (texto) em números**"""

enc = OrdinalEncoder()
Xc_train = pd.DataFrame(enc.fit_transform(X_train), columns=X_train.columns, dtype=int)
Xc_train # coloquei Xc categorical, mas aqui todas as variáveis preditivas são categóricas

print('Xc_train.shape: ', Xc_train.shape)

enc.categories_

"""Aqui, vemos a ordem das categorias:

* JOB: 0: admin, 1: blue-colar, 2: entrepreneur...

* MARITAL: 0: divorced, 1: married, 2: single...

e assim por diante.


"""

Xc_test = pd.DataFrame(enc.transform(X_test), columns=X_test.columns, dtype=int)
'''
perceba que aqui foi usado transform e não fit_transform.
fit_transform cria as regras e as aplica ao dataframe.
transform utiliza as regras já criadas anteriormente.
queremos que as categorias aplicadas para X_train 
sejam as mesmas para X_test.
'''
Xc_test

print('Xc_test.shape: ', Xc_test.shape)

"""Se acontecer de uma categoria nova surgir no grupo de testes (inexistente no grupo de treino), por padrão, vai dar erro.

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html

Podemos ver, na biblioteca, que é possível criar as classes de antemão - não deixar que elas sejam criadas automaticamente durante o treino.

Como pode ser é possível que apareça uma profissão imprevista, podemos utilizar a variável 'handle_unknown' para setar um valor.

Porém, veremos como lidar com esses problemas em outro módulo, o de data prep.

Seguindo em frente... vamos agora trabalhar com a variável resposta, Y.
"""

enc2 = LabelEncoder() #criar outro encoder para Y, que não é matriz
Yc_train = enc2.fit_transform(Y_train)
Yc_train

print('Yc_train.shape: ', Yc_train.shape)

"""Agora transformamos "no" em 0 e "yes" em 1.

Vamos fazer o mesmo para o conjunto de testes.
"""

Yc_test = enc2.transform(Y_test)
Yc_test

print('Yc_test.shape: ', Yc_test.shape)

"""**Treinar o modelo**"""

# categorical naive bayes 
cnb2 = CategoricalNB(alpha=0) # depois veremos o que alpha significa
cnb2.fit(Xc_train, Yc_train)

"""**Testar e avaliar o modelo**"""

pred2 = cnb2.predict_proba(Xc_test)
pred2

print('pred2.shape: ', pred2.shape)

plt.figure(figsize=(15,5))
plt.hist(pred2[Yc_test==0,1], bins=np.linspace(0,1,30), color='r', alpha=0.3, rwidth=0.8, density=True, label='não comprou') # histograma só de quem não comprou 
plt.hist(pred2[Yc_test==1,1], bins=np.linspace(0,1,30), color='g', alpha=0.3, rwidth=0.8, density=True, label='comprou') # histograma só de quem comprou
plt.legend()
plt.grid();

"""Pelo gráfico acima, podemos ver que para score acima de 0.2, são os clientes que compram.

**Significado de alpha em CategoricalNB**

Serve para lidar com respostas novas ou respostas talvez inválidas. Por exemplo, em "casado", só temos respostas "sim" e "não". Mas um cliente pode responder "talvez" ou "namorando".

Então é necessário atualizar os valores das probabilidades de cada resposta possível, caso eu queira considerar a nova resposta como válida e provável.

Retomemos as tabelas iniciais para X e Y, com apenas 8 linhas.
"""

X = df_pre[['casado']]
X

Y = df_pre.comprou
Y

"""Refazer o treinamento, com outro alpha."""

cnb = CategoricalNB(alpha=0, min_categories=3) # agora podemos esperar que apareçam 3 categorias: não, sim, talvez (ou outra coisa)]
cnb.fit(X,Y)

pred3 = pd.DataFrame(columns=['casado'])
pred3.loc[0,:] = [2] # criar um novo dado, com uma categoria nunca vista antes

cnb.predict_proba(pred3)

"""Como a categoria 2 (das existentes 0, 1 e agora 2) nunca foi vista antes, a resposta que ele dá é 0.5 de chance para comprar e 0.5 de chance para não comprar.

Porém, sabemos que, independentemente de estar casado ou não, a chance de o cliente novo comprar é maior do que de não comprar (5/8 para comprar e 3/8 para não comprar).

Então faz sentido distribuir igualmente a chance para um caso nunca visto?
"""

# Treinar um novo modelo, agora com alpha=50 (escolhi qualquer número)
cnb = CategoricalNB(alpha=50, min_categories=3)
cnb.fit(X,Y)

cnb.predict_proba(pred3)

"""Podemos ver agora que a chance de não comprar é menor do que a de comprar. Isso está mais próximo do que os dados conhecidos nos mostram.

No futuro, aprenderemos sobre o grid search, que basicamente testa vários hiperparâmetros (entre eles, o alpha) para escolher o melhor.
"""